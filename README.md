# ğŸ” Deep Search Agent - Desafio MySide

## ğŸ“Œ DescriÃ§Ã£o do Projeto
Este projeto foi desenvolvido para realizar buscas detalhadas sobre pessoas na internet utilizando um agente de pesquisa com suporte de LLMs (Large Language Models). O objetivo principal Ã© reunir informaÃ§Ãµes de diversas fontes, como:
- **Google** (via SerpAPI)
- **Redes Sociais** (LinkedIn, Facebook, Instagram)
- **NotÃ­cias** (via NewsAPI)

O projeto faz uso do **LangChain**, **LlamaCpp**, **FastAPI** e **SQLAlchemy**, permitindo buscas eficientes e organizaÃ§Ã£o dos dados coletados em um formato estruturado.

---

## ğŸ›  Tecnologias Utilizadas
- **Python 3.10+**
- **FastAPI** - Framework para APIs rÃ¡pidas
- **LangChain** - Framework para interaÃ§Ã£o com LLMs
- **LlamaCpp** - ExecuÃ§Ã£o de modelos LLM localmente
- **SQLAlchemy** - ORM para persistÃªncia de dados
- **SerpAPI** - API para busca no Google
- **NewsAPI** - API para busca de notÃ­cias
- **Uvicorn** - Servidor ASGI para FastAPI

---

## ğŸš€ Como Rodar o Projeto Localmente

### ğŸ“‚ 1. Clonar o repositÃ³rio
```bash
git clone https://github.com/mauriciobedun/desafio-myside.git
cd desafio-myside
```

### ğŸ“¦ 2. Criar e ativar um ambiente virtual
```bash
python -m venv Desafio-MySide
source Desafio-MySide/bin/activate  # Linux/Mac
Desafio-MySide\Scripts\activate     # Windows
```

### ğŸ“¥ 3. Instalar dependÃªncias
```bash
pip install -r requirements.txt
```

### ğŸ”‘ 4. Configurar as Chaves de API
Edite o arquivo core/config.py e adicione suas chaves da SerpAPI e NewsAPI:
```python
serpapi_key: str = "SUA_CHAVE_SERPAPI"
newsapi_key: str = "SUA_CHAVE_NEWSAPI"
```

Se necessÃ¡rio, configure o caminho do modelo Llama:
```python
LLAMA_MODEL_PATH: str = "./models/llama-2-7b.Q4_K_M.gguf"
```

### â–¶ï¸ 5. Rodar o projeto
```bash
uvicorn app.main:app --reload
```

A API estarÃ¡ disponÃ­vel em:
```ccp
http://127.0.0.1:8000
```

## ğŸ” Como Utilizar
A API oferece um endpoint /buscar que aceita dois parÃ¢metros: nome e telefone.

### ğŸ“Œ Exemplo de requisiÃ§Ã£o:
```bash
curl "http://127.0.0.1:8000/buscar?nome=mauricio%20bedun&telefone=11989731384"
```

### ğŸ“Œ Exemplo de resposta:
```json
{
  "deep_search_data": {
    "nome_completo": "Mauricio Bedun",
    "idade": 25,
    "genero": "masculino",
    "estado_civil": "solteiro",
    "localizacao": "SÃ£o Paulo",
    "profissao": "Engenheiro eletrÃ´nico",
    "empresa_atual": "CMEI Maria Silva Santos",
    "redes_sociais": {
      "linkedin": "https://www.linkedin.com/in/mauricio-bedun-jr-92630415b/",
      "facebook": "https://www.facebook.com/mauricio.bedun",
      "instagram": "https://www.instagram.com/mauriciobedun/"
    },
    "interesses": ["esportes", "cine"],
    "noticias": []
  },
  "raw_output": "=== GOOGLE ===\nErro ao realizar busca no Google: Got error from SerpAPI: Google hasn't returned any results for this query...",
  "metadata": {
    "nome": "mauricio bedun",
    "telefone": "11989731384",
    "timestamp": "2025-03-17T18:16:04.712949"
  }
}
```

## Desafios Enfrentados
### 1ï¸âƒ£ AlucinaÃ§Ã£o da LLM
Os modelos LLM podem "alucinar" (inventar informaÃ§Ãµes), entÃ£o algumas estratÃ©gias foram usadas para minimizar esse problema:

- Abordagem em 2 etapas:
    - Primeiro, geramos um resumo das informaÃ§Ãµes coletadas.
    - Depois, pedimos apenas o JSON formatado, reduzindo a chance de respostas inconsistentes.
### 2ï¸âƒ£ Tempo Escasso (Apenas 4 dias)
O projeto foi desenvolvido sob um prazo curto, o que dificultou a implementaÃ§Ã£o de um Web Scraping manual. Em vez disso, utilizamos APIs externas (SerpAPI, NewsAPI) para obter resultados rapidamente.

### 3ï¸âƒ£ Tamanho do Texto para LLM
Se o texto de entrada for muito grande, o modelo pode ficar confuso ou truncar respostas. Para contornar isso:

- Limitamos os resultados (exemplo: num=2 nas buscas)
- Filtramos informaÃ§Ãµes irrelevantes antes de enviar ao LLM

## ğŸ“„ Estrutura do Projeto

```bash
ğŸ“‚ Desafio-myside
â”‚â”€â”€ ğŸ“‚ agents             # ImplementaÃ§Ã£o dos agentes de busca
â”‚   â”‚â”€â”€ deep_search_agent.py
â”‚   â”‚â”€â”€ orchestrator.py
â”‚â”€â”€ ğŸ“‚ app                # AplicaÃ§Ã£o FastAPI
â”‚   â”‚â”€â”€ main.py
â”‚â”€â”€ ğŸ“‚ core               # ConfiguraÃ§Ãµes e banco de dados
â”‚   â”‚â”€â”€ config.py
â”‚   â”‚â”€â”€ database.py
â”‚   â”‚â”€â”€ llama_loader.py
â”‚   â”‚â”€â”€ models.py
â”‚â”€â”€ requirements.txt      # DependÃªncias do projeto
â”‚â”€â”€ README.md             # Este arquivo
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ .env
â”‚â”€â”€ .gitignore
â”‚â”€â”€ llama-2-13b.Q4_K_M.gguf
```

## ğŸ ConclusÃ£o
Este projeto foi desenvolvido em um prazo extremamente curto (4 dias), o que impÃ´s desafios significativos na coleta e processamento de informaÃ§Ãµes. Criar um sistema de web scraping manual para diversas fontes pÃºblicas seria inviÃ¡vel nesse perÃ­odo, entÃ£o optamos por usar APIs externas como SerpAPI e NewsAPI, alÃ©m de um modelo de LLM local (LlamaCpp) para estruturar os dados.

Um dos principais desafios foi a dificuldade de encontrar informaÃ§Ãµes completas e confiÃ¡veis. Devido Ã  LGPD (Lei Geral de ProteÃ§Ã£o de Dados), muitos dados pessoais nÃ£o estÃ£o publicamente disponÃ­veis, e algumas buscas retornaram resultados limitados ou imprecisos. Isso tambÃ©m gerou o problema de alucinaÃ§Ã£o da IA, onde o modelo pode "inventar" informaÃ§Ãµes baseadas em padrÃµes, mas sem confirmaÃ§Ã£o real.

Outro ponto crÃ­tico foi o uso do Llama (modelo open-source), que, apesar de ser uma alternativa acessÃ­vel, apresentou mais dificuldades na extraÃ§Ã£o precisa de JSON do que um modelo proprietÃ¡rio como o da OpenAI (GPT-4). Modelos pagos possuem um melhor ajuste para recuperaÃ§Ã£o de informaÃ§Ãµes estruturadas, reduzindo a alucinaÃ§Ã£o e aumentando a confiabilidade dos dados retornados.

Mesmo com essas limitaÃ§Ãµes, conseguimos criar um sistema funcional que busca, filtra e estrutura informaÃ§Ãµes de mÃºltiplas fontes. As otimizaÃ§Ãµes implementadas no prompt engineering e no processamento dos resultados ajudaram a mitigar alguns dos problemas de inconsistÃªncia.

Se tiver sugestÃµes ou melhorias, sinta-se Ã  vontade para contribuir! ğŸš€